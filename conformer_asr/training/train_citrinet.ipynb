{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from omegaconf import OmegaConf, open_dict\n",
    "import nemo.collections.asr as nemo_asr\n",
    "from nemo.collections.asr.metrics.wer import word_error_rate\n",
    "from nemo.utils import exp_manager\n",
    "import nemo\n",
    "from omegaconf import OmegaConf\n",
    "# Manifest Utils\n",
    "from tqdm.auto import tqdm\n",
    "import json\n",
    "# Preprocessing steps\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as ptl\n",
    "import json\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "import copy\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "WANDB_LOGGER = True\n",
    "ASR_DIR = \"/home/khoatlv/ASR-NEMO\"\n",
    "tokenizer_dir = os.path.join(ASR_DIR, \"tokenizers\", \"tokenizers_citrinet\")\n",
    "TOKENIZER_TYPE = \"bpe\" #@param [\"bpe\", \"unigram\"]\n",
    "\n",
    "model_config = \"config/citrinet_256.yaml\"\n",
    "config_path = os.path.join(ASR_DIR, model_config)\n",
    "\n",
    "train_manifest_cleaned = \"/home/khoatlv/manifests/train_manifest_processed.json\"\n",
    "test_manifest_cleaned = \"/home/khoatlv/manifests/test_manifest_processed.json\"\n",
    "\n",
    "train_set = None\n",
    "test_set = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_config(path):\n",
    "    config = OmegaConf.load(path)\n",
    "    config = OmegaConf.to_container(config, resolve=True)\n",
    "    config = OmegaConf.create(config)\n",
    "    \n",
    "    config.model.train_ds.manifest_filepath = train_manifest_cleaned\n",
    "    config.model.validation_ds.manifest_filepath = test_manifest_cleaned\n",
    "    config.model.test_ds.manifest_filepath = test_manifest_cleaned\n",
    "    \n",
    "    # config.model.train_ds.labels = list(train_set)\n",
    "    # config.model.validation_ds.labels = list(train_set)\n",
    "    # config.model.test_ds.labels = list(train_set)\n",
    "    \n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:root:Corpus already exists at path : /home/khoatlv/ASR-NEMO/tokenizers_citrinet/text_corpus/document.txt\n",
      "[NeMo I 2022-03-28 10:11:43 sentencepiece_tokenizer:307] Processing /home/khoatlv/ASR-NEMO/tokenizers_citrinet/text_corpus/document.txt and store at /home/khoatlv/ASR-NEMO/tokenizers_citrinet/tokenizer_spe_bpe_v512\n",
      "sentencepiece_trainer.cc(177) LOG(INFO) Running command: --input=/home/khoatlv/ASR-NEMO/tokenizers_citrinet/text_corpus/document.txt --model_prefix=/home/khoatlv/ASR-NEMO/tokenizers_citrinet/tokenizer_spe_bpe_v512/tokenizer --vocab_size=512 --shuffle_input_sentence=true --hard_vocab_limit=false --model_type=bpe --character_coverage=1.0 --bos_id=-1 --eos_id=-1\n",
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: /home/khoatlv/ASR-NEMO/tokenizers_citrinet/text_corpus/document.txt\n",
      "  input_format: \n",
      "  model_prefix: /home/khoatlv/ASR-NEMO/tokenizers_citrinet/tokenizer_spe_bpe_v512/tokenizer\n",
      "  model_type: BPE\n",
      "  vocab_size: 512\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 1\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 0\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: -1\n",
      "  eos_id: -1\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(329) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(178) LOG(INFO) Loading corpus: /home/khoatlv/ASR-NEMO/tokenizers_citrinet/text_corpus/document.txt\n",
      "trainer_interface.cc(385) LOG(INFO) Loaded all 160195 sentences\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(405) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(466) LOG(INFO) all chars count=11784327\n",
      "trainer_interface.cc(487) LOG(INFO) Alphabet size=94\n",
      "trainer_interface.cc(488) LOG(INFO) Final character coverage=1\n",
      "trainer_interface.cc(520) LOG(INFO) Done! preprocessed 160195 sentences.\n",
      "trainer_interface.cc(526) LOG(INFO) Tokenizing input sentences with whitespace: 160195\n",
      "trainer_interface.cc(537) LOG(INFO) Done! 7362\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=434371 min_freq=138\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=76825 size=20 all=3076 active=2211 piece=▁g\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=32055 size=40 all=3651 active=2786 piece=▁x\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=23173 size=60 all=3935 active=3070 piece=▁người\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=17708 size=80 all=4203 active=3338 piece=▁tôi\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=14518 size=100 all=4451 active=3586 piece=▁con\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=14264 min_freq=503\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=12110 size=120 all=4620 active=1169 piece=▁bạn\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=10157 size=140 all=4837 active=1386 piece=ực\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=9023 size=160 all=5066 active=1615 piece=ăn\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=7712 size=180 all=5272 active=1821 piece=▁nào\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=6879 size=200 all=5420 active=1969 piece=▁mươi\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=6792 min_freq=504\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=6127 size=220 all=5583 active=1164 piece=▁y\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=5626 size=240 all=5706 active=1287 piece=▁nay\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=5245 size=260 all=5802 active=1383 piece=iệu\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=4738 size=280 all=5914 active=1495 piece=▁mẹ\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=4371 size=300 all=6021 active=1602 piece=ọng\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=4366 min_freq=440\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=4104 size=320 all=6089 active=1058 piece=▁lo\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=3811 size=340 all=6225 active=1194 piece=▁chi\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=3626 size=360 all=6298 active=1267 piece=▁viên\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=3344 size=380 all=6402 active=1371 piece=▁tổ\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=3113 size=400 all=6526 active=1495 piece=ởi\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=3102 min_freq=368\n",
      "trainer_interface.cc(615) LOG(INFO) Saving model: /home/khoatlv/ASR-NEMO/tokenizers_citrinet/tokenizer_spe_bpe_v512/tokenizer.model\n",
      "trainer_interface.cc(626) LOG(INFO) Saving vocabs: /home/khoatlv/ASR-NEMO/tokenizers_citrinet/tokenizer_spe_bpe_v512/tokenizer.vocab\n",
      "Serialized tokenizer at location : /home/khoatlv/ASR-NEMO/tokenizers_citrinet/tokenizer_spe_bpe_v512\n",
      "INFO:root:Done!\n"
     ]
    }
   ],
   "source": [
    "VOCAB_SIZE = 512\n",
    "!python3 scripts/process_asr_text_tokenizer.py \\\n",
    "  --manifest=$train_manifest_cleaned \\\n",
    "  --vocab_size=$VOCAB_SIZE \\\n",
    "  --data_root=$tokenizer_dir \\\n",
    "  --tokenizer=\"spe\" \\\n",
    "  --spe_type=$TOKENIZER_TYPE \\\n",
    "  --spe_character_coverage=1.0 \\\n",
    "  --no_lower_case \\\n",
    "  --log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer directory : /home/khoatlv/ASR-NEMO/tokenizers_citrinet/tokenizer_spe_bpe_v512/\n",
      "Number of tokens :  512\n"
     ]
    }
   ],
   "source": [
    "TOKENIZER_DIR = f\"{tokenizer_dir}/tokenizer_spe_{TOKENIZER_TYPE}_v{VOCAB_SIZE}/\"\n",
    "print(\"Tokenizer directory :\", TOKENIZER_DIR)\n",
    "\n",
    "# Number of tokens in tokenizer - \n",
    "with open(os.path.join(TOKENIZER_DIR, 'tokenizer.vocab')) as f:\n",
    "  tokens = f.readlines()\n",
    "\n",
    "num_tokens = len(tokens)\n",
    "print(\"Number of tokens : \", num_tokens)\n",
    "if num_tokens < VOCAB_SIZE:\n",
    "    print(\n",
    "        f\"The text in this dataset is too small to construct a tokenizer \"\n",
    "        f\"with vocab size = {VOCAB_SIZE}. Current number of tokens = {num_tokens}. \"\n",
    "        f\"Please reconstruct the tokenizer with fewer tokens\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asr_model = nemo_asr.models.ASRModel.from_pretrained(\"stt_en_citrinet_256\", map_location='cuda')\n",
    "# asr_model.change_vocabulary(new_tokenizer_dir=TOKENIZER_DIR, new_tokenizer_type=\"bpe\")\n",
    "print(asr_model.cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2022-03-31 04:32:56 nemo_logging:349] /tmp/ipykernel_3138791/1770712708.py:1: LightningDeprecationWarning: The `LightningModule.summarize` method is deprecated in v1.5 and will be removed in v1.7. Use `pytorch_lightning.utilities.model_summary.summarize` instead.\n",
      "      asr_model.summarize()\n",
      "    \n",
      "[NeMo W 2022-03-31 04:32:56 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/pytorch_lightning/utilities/model_summary.py:471: LightningDeprecationWarning: Argument `mode` in `LightningModule.summarize` is deprecated in v1.4 and will be removed in v1.6. Use `max_depth=1` to replicate `mode=top` behavior.\n",
      "      rank_zero_deprecation(\n",
      "    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "  | Name              | Type                              | Params\n",
       "------------------------------------------------------------------------\n",
       "0 | preprocessor      | AudioToMelSpectrogramPreprocessor | 0     \n",
       "1 | encoder           | ConvASREncoder                    | 36.3 M\n",
       "2 | decoder           | ConvASRDecoder                    | 657 K \n",
       "3 | loss              | CTCLoss                           | 0     \n",
       "4 | spec_augmentation | SpectrogramAugmentation           | 0     \n",
       "5 | _wer              | WERBPE                            | 0     \n",
       "------------------------------------------------------------------------\n",
       "37.0 M    Trainable params\n",
       "0         Non-trainable params\n",
       "37.0 M    Total params\n",
       "147.977   Total estimated model params size (MB)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asr_model.summarize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2022-03-28 10:12:45 collections:173] Dataset loaded with 158604 files totalling 183.65 hours\n",
      "[NeMo I 2022-03-28 10:12:45 collections:174] 1591 files were filtered totalling 10.04 hours\n",
      "[NeMo I 2022-03-28 10:12:46 collections:173] Dataset loaded with 17416 files totalling 20.85 hours\n",
      "[NeMo I 2022-03-28 10:12:46 collections:174] 0 files were filtered totalling 0.00 hours\n",
      "[NeMo I 2022-03-28 10:12:47 collections:173] Dataset loaded with 17416 files totalling 20.85 hours\n",
      "[NeMo I 2022-03-28 10:12:47 collections:174] 0 files were filtered totalling 0.00 hours\n"
     ]
    }
   ],
   "source": [
    "config = load_config(model_config)\n",
    "\n",
    "cfg = copy.deepcopy(asr_model.cfg)\n",
    "# Setup new tokenizer\n",
    "cfg.tokenizer.dir = TOKENIZER_DIR\n",
    "cfg.tokenizer.type = \"bpe\"\n",
    "\n",
    "# Set tokenizer config\n",
    "asr_model.cfg.tokenizer = cfg.tokenizer\n",
    "\n",
    "# Setup train, validation, test configs\n",
    "with open_dict(cfg):\n",
    "  # Train dataset\n",
    "  cfg.train_ds.manifest_filepath = train_manifest_cleaned\n",
    "  cfg.train_ds.batch_size = 32\n",
    "  cfg.train_ds.num_workers = 8\n",
    "  cfg.train_ds.pin_memory = True\n",
    "  cfg.train_ds.use_start_end_token = True\n",
    "  cfg.train_ds.trim_silence = True\n",
    "\n",
    "  # Validation dataset\n",
    "  cfg.validation_ds.manifest_filepath = test_manifest_cleaned\n",
    "  cfg.validation_ds.batch_size = 8\n",
    "  cfg.validation_ds.num_workers = 8\n",
    "  cfg.validation_ds.pin_memory = True\n",
    "  cfg.validation_ds.use_start_end_token = True\n",
    "  cfg.validation_ds.trim_silence = True\n",
    "\n",
    "  # Test dataset\n",
    "  cfg.test_ds.manifest_filepath = test_manifest_cleaned\n",
    "  cfg.test_ds.batch_size = 8\n",
    "  cfg.test_ds.num_workers = 8\n",
    "  cfg.test_ds.pin_memory = True\n",
    "  cfg.test_ds.use_start_end_token = True\n",
    "  cfg.test_ds.trim_silence = True\n",
    "\n",
    "# setup model with new configs\n",
    "asr_model.setup_training_data(cfg.train_ds)\n",
    "asr_model.setup_multiple_validation_data(cfg.validation_ds)\n",
    "asr_model.setup_multiple_test_data(cfg.test_ds)\n",
    "\n",
    "with open_dict(asr_model.cfg.optim):\n",
    "  asr_model.cfg.optim.lr = 0.025\n",
    "  asr_model.cfg.optim.weight_decay = 0.001\n",
    "  asr_model.cfg.optim.sched.warmup_steps = None  # Remove default number of steps of warmup\n",
    "  asr_model.cfg.optim.sched.warmup_ratio = 0.10  # 10 % warmup\n",
    "  asr_model.cfg.optim.sched.min_lr = 1e-9\n",
    "\n",
    "with open_dict(asr_model.cfg.spec_augment):\n",
    "  asr_model.cfg.spec_augment.freq_masks = 2\n",
    "  asr_model.cfg.spec_augment.freq_width = 27\n",
    "  asr_model.cfg.spec_augment.time_masks = 2\n",
    "  asr_model.cfg.spec_augment.time_width = 0.05\n",
    "\n",
    "asr_model.spec_augmentation = asr_model.from_config_dict(asr_model.cfg.spec_augment)\n",
    "\n",
    "asr_model._wer.use_cer = True\n",
    "asr_model._wer.log_prediction = True\n",
    "\n",
    "trainer = ptl.Trainer(**config.trainer)\n",
    "asr_model.set_trainer(trainer)\n",
    "asr_model.cfg = asr_model._cfg\n",
    "\n",
    "exp_config = exp_manager.ExpManagerConfig(**config.exp_manager)\n",
    "exp_config = OmegaConf.structured(exp_config)\n",
    "logdir = exp_manager.exp_manager(trainer, exp_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf9175f8eb204596a99c6636f0e261d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Checking for CTC failures:   0%|          | 0/4957 [00:01<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CTC loss will fail for 33 samples (0.020806537035635925 % of samples)!\n",
      "Increase the vocabulary size of the tokenizer so that this number becomes close to zero !\n",
      "Average Acoustic model sequence length = 52.62409523088951\n",
      "Average Target sequence length = 24.510686994022848\n",
      "\n",
      "Ratio of Average AM sequence length to target sequence length = 2.344184373505856\n"
     ]
    }
   ],
   "source": [
    "def analyse_ctc_failures_in_model(model):\n",
    "    count_ctc_failures = 0\n",
    "    am_seq_lengths = []\n",
    "    target_seq_lengths = []\n",
    "\n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "    model = model.to(device)\n",
    "    mode = model.training\n",
    "    \n",
    "    train_dl = model.train_dataloader()\n",
    "\n",
    "    with torch.no_grad():\n",
    "      model = model.eval()\n",
    "      for batch in tqdm(train_dl, desc='Checking for CTC failures'):\n",
    "          x, x_len, y, y_len = batch\n",
    "          x, x_len = x.to(device), x_len.to(device)\n",
    "          x_logprobs, x_len, greedy_predictions = model(input_signal=x, input_signal_length=x_len)\n",
    "\n",
    "          # Find how many CTC loss computation failures will occur\n",
    "          for xl, yl in zip(x_len, y_len):\n",
    "              if xl <= yl:\n",
    "                  count_ctc_failures += 1\n",
    "\n",
    "          # Record acoustic model lengths=\n",
    "          am_seq_lengths.extend(x_len.to('cpu').numpy().tolist())\n",
    "\n",
    "          # Record target sequence lengths\n",
    "          target_seq_lengths.extend(y_len.to('cpu').numpy().tolist())\n",
    "          \n",
    "          del x, x_len, y, y_len, x_logprobs, greedy_predictions\n",
    "    \n",
    "    if mode:\n",
    "      model = model.train()\n",
    "      \n",
    "    return count_ctc_failures, am_seq_lengths, target_seq_lengths\n",
    "\n",
    "results = analyse_ctc_failures_in_model(asr_model)\n",
    "num_ctc_failures, am_seq_lengths, target_seq_lengths = results\n",
    "if num_ctc_failures > 0:\n",
    "  print(f\"\\nCTC loss will fail for {num_ctc_failures} samples ({num_ctc_failures * 100./ float(len(am_seq_lengths))} % of samples)!\\n\"\n",
    "                  f\"Increase the vocabulary size of the tokenizer so that this number becomes close to zero !\")\n",
    "else:\n",
    "  print(\"No CTC failure cases !\")\n",
    "# Compute average ratio of T / U\n",
    "avg_T = sum(am_seq_lengths) / float(len(am_seq_lengths))\n",
    "avg_U = sum(target_seq_lengths) / float(len(target_seq_lengths))\n",
    "\n",
    "avg_length_ratio = 0\n",
    "for am_len, tgt_len in zip(am_seq_lengths, target_seq_lengths):\n",
    "  avg_length_ratio += (am_len / float(tgt_len))\n",
    "avg_length_ratio = avg_length_ratio / len(am_seq_lengths)\n",
    "\n",
    "print(f\"Average Acoustic model sequence length = {avg_T}\")\n",
    "print(f\"Average Target sequence length = {avg_U}\")\n",
    "print()\n",
    "print(f\"Ratio of Average AM sequence length to target sequence length = {avg_length_ratio}\")\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
