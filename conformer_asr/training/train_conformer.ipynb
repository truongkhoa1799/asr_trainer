{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nemo.collections.asr as nemo_asr\n",
    "from omegaconf import OmegaConf, open_dict\n",
    "import pytorch_lightning as ptl\n",
    "from nemo.utils import exp_manager\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import sys\n",
    "import time\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "ASR_DIR = \"/home/khoatlv\"\n",
    "sys.path.append(ASR_DIR)\n",
    "from Conformer_ASR.scripts.utils import config, Logger, Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "183819 /home/khoatlv/manifests/train_manifest_processed.json\n",
      "22184 /home/khoatlv/manifests/test_manifest_processed.json\n",
      "Tokenizer directory : /home/khoatlv/Conformer_ASR/tokenizers/tokenizers_conformer_1654996267\n"
     ]
    }
   ],
   "source": [
    "ASR_DIR = \"/home/khoatlv/Conformer_ASR\"\n",
    "\n",
    "tokenizer_cfg = config.get_config([\"training\", \"tokenizer\"])\n",
    "tokenizer_dir = tokenizer_cfg.tokenizer_dir + \"_\" + str(int(round(time.time(), 0)))\n",
    "vocab_size = config.get_config([\"training\", \"vocab_size\"])\n",
    "type = tokenizer_cfg.type                # can be wpe or spe\n",
    "type_cfg = tokenizer_cfg.type_cfg        # [\"bpe\", \"unigram\"]\n",
    "\n",
    "train_manifest_cleaned = \"/home/khoatlv/manifests/train_manifest_processed.json\"\n",
    "test_manifest_cleaned = \"/home/khoatlv/manifests/test_manifest_processed.json\"\n",
    "\n",
    "! wc -l {train_manifest_cleaned}\n",
    "! wc -l {test_manifest_cleaned}\n",
    "\n",
    "# Tokenizer path\n",
    "print(\"Tokenizer directory :\", tokenizer_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:root:Finished extracting manifest : /home/khoatlv/manifests/train_manifest_processed.json\n",
      "INFO:root:Finished extracting all manifests ! Number of sentences : 183819\n",
      "[NeMo I 2022-06-12 01:11:15 sentencepiece_tokenizer:307] Processing /home/khoatlv/Conformer_ASR/tokenizers/tokenizers_conformer_1654996267/text_corpus/document.txt and store at /home/khoatlv/Conformer_ASR/tokenizers/tokenizers_conformer_1654996267/tokenizer_spe_bpe_v256\n",
      "sentencepiece_trainer.cc(177) LOG(INFO) Running command: --input=/home/khoatlv/Conformer_ASR/tokenizers/tokenizers_conformer_1654996267/text_corpus/document.txt --model_prefix=/home/khoatlv/Conformer_ASR/tokenizers/tokenizers_conformer_1654996267/tokenizer_spe_bpe_v256/tokenizer --vocab_size=256 --shuffle_input_sentence=true --hard_vocab_limit=false --model_type=bpe --character_coverage=1.0 --bos_id=-1 --eos_id=-1\n",
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: /home/khoatlv/Conformer_ASR/tokenizers/tokenizers_conformer_1654996267/text_corpus/document.txt\n",
      "  input_format: \n",
      "  model_prefix: /home/khoatlv/Conformer_ASR/tokenizers/tokenizers_conformer_1654996267/tokenizer_spe_bpe_v256/tokenizer\n",
      "  model_type: BPE\n",
      "  vocab_size: 256\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 1\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 0\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: -1\n",
      "  eos_id: -1\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(329) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(178) LOG(INFO) Loading corpus: /home/khoatlv/Conformer_ASR/tokenizers/tokenizers_conformer_1654996267/text_corpus/document.txt\n",
      "trainer_interface.cc(385) LOG(INFO) Loaded all 183819 sentences\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(405) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(466) LOG(INFO) all chars count=12960959\n",
      "trainer_interface.cc(487) LOG(INFO) Alphabet size=94\n",
      "trainer_interface.cc(488) LOG(INFO) Final character coverage=1\n",
      "trainer_interface.cc(520) LOG(INFO) Done! preprocessed 183819 sentences.\n",
      "trainer_interface.cc(526) LOG(INFO) Tokenizing input sentences with whitespace: 183819\n",
      "trainer_interface.cc(537) LOG(INFO) Done! 7408\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=476945 min_freq=158\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=85122 size=20 all=3127 active=2256 piece=▁g\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=34888 size=40 all=3713 active=2842 piece=ột\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=24939 size=60 all=3982 active=3111 piece=▁người\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=19310 size=80 all=4224 active=3353 piece=▁trong\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=15755 size=100 all=4479 active=3608 piece=ương\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=15608 min_freq=589\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=13032 size=120 all=4638 active=1138 piece=▁họ\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=11474 size=140 all=4836 active=1336 piece=ức\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=9778 size=160 all=5064 active=1564 piece=ồng\n",
      "trainer_interface.cc(615) LOG(INFO) Saving model: /home/khoatlv/Conformer_ASR/tokenizers/tokenizers_conformer_1654996267/tokenizer_spe_bpe_v256/tokenizer.model\n",
      "trainer_interface.cc(626) LOG(INFO) Saving vocabs: /home/khoatlv/Conformer_ASR/tokenizers/tokenizers_conformer_1654996267/tokenizer_spe_bpe_v256/tokenizer.vocab\n",
      "Serialized tokenizer at location : /home/khoatlv/Conformer_ASR/tokenizers/tokenizers_conformer_1654996267/tokenizer_spe_bpe_v256\n",
      "INFO:root:Done!\n"
     ]
    }
   ],
   "source": [
    "!python3 tokenizers/process_asr_text_tokenizer.py \\\n",
    "   --manifest=$train_manifest_cleaned \\\n",
    "   --data_root=$tokenizer_dir \\\n",
    "   --tokenizer=$type \\\n",
    "   --spe_type=$type_cfg \\\n",
    "   --spe_character_coverage=1.0 \\\n",
    "   --no_lower_case \\\n",
    "   --log \\\n",
    "   --vocab_size=$vocab_size\n",
    "# ------------------------------------------------------------------- #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_config(path):\n",
    "    config = OmegaConf.load(path)\n",
    "    config = OmegaConf.to_container(config, resolve=True)\n",
    "    config = OmegaConf.create(config)\n",
    "    \n",
    "    config.model.train_ds.manifest_filepath = train_manifest_cleaned\n",
    "    config.model.validation_ds.manifest_filepath = test_manifest_cleaned\n",
    "    config.model.test_ds.manifest_filepath = test_manifest_cleaned\n",
    "    \n",
    "    return config\n",
    "\n",
    "def enable_bn_se(m):\n",
    "    if type(m) == nn.BatchNorm1d:\n",
    "        m.train()\n",
    "        for param in m.parameters():\n",
    "            param.requires_grad_(True)\n",
    "\n",
    "    if 'SqueezeExcite' in type(m).__name__:\n",
    "        m.train()\n",
    "        for param in m.parameters():\n",
    "            param.requires_grad_(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2022-05-27 15:10:18 mixins:146] Tokenizer SentencePieceTokenizer initialized with 256 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2022-05-27 15:10:19 modelPT:148] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
      "    Train config : \n",
      "    manifest_filepath: /home/nhan/NovaIntechs/data/ASR_Data/manifests/train_manifest_processed.json\n",
      "    sample_rate: 16000\n",
      "    max_duration: 16.7\n",
      "    min_duration: 0.1\n",
      "    is_tarred: false\n",
      "    tarred_audio_filepaths: null\n",
      "    shuffle_n: 2048\n",
      "    bucketing_strategy: synced_randomized\n",
      "    bucketing_batch_size: null\n",
      "    shuffle: true\n",
      "    batch_size: 32\n",
      "    pin_memory: true\n",
      "    trim_silence: true\n",
      "    use_start_end_token: true\n",
      "    normalize_transcripts: false\n",
      "    num_workers: 16\n",
      "    \n",
      "[NeMo W 2022-05-27 15:10:19 modelPT:155] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
      "    Validation config : \n",
      "    manifest_filepath: /home/nhan/NovaIntechs/data/ASR_Data/manifests/test_manifest_processed.json\n",
      "    sample_rate: 16000\n",
      "    pin_memory: true\n",
      "    shuffle: false\n",
      "    batch_size: 8\n",
      "    trim_silence: true\n",
      "    use_start_end_token: true\n",
      "    normalize_transcripts: false\n",
      "    num_workers: 16\n",
      "    \n",
      "[NeMo W 2022-05-27 15:10:19 modelPT:161] Please call the ModelPT.setup_test_data() or ModelPT.setup_multiple_test_data() method and provide a valid configuration file to setup the test data loader(s).\n",
      "    Test config : \n",
      "    manifest_filepath: /home/nhan/NovaIntechs/data/ASR_Data/manifests/test_manifest_processed.json\n",
      "    sample_rate: 16000\n",
      "    pin_memory: true\n",
      "    shuffle: false\n",
      "    batch_size: 8\n",
      "    trim_silence: true\n",
      "    use_start_end_token: true\n",
      "    normalize_transcripts: false\n",
      "    num_workers: 16\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2022-05-27 15:10:19 features:255] PADDING: 0\n",
      "[NeMo I 2022-05-27 15:10:19 features:272] STFT using torch\n",
      "[NeMo I 2022-05-27 15:10:40 save_restore_connector:157] Model EncDecCTCModelBPE was successfully restored from /home/khoatlv/Conformer_ASR/models/conformer/Conformer_small_epoch=98.nemo.\n"
     ]
    }
   ],
   "source": [
    "# asr_model = nemo_asr.models.EncDecCTCModelBPE.from_pretrained(model_name=\"stt_en_conformer_ctc_small\",map_location='cuda')\n",
    "# asr_model.change_vocabulary(new_tokenizer_dir=TOKENIZER_DIR, new_tokenizer_type=TOKENIZER_TYPE_CFG)\n",
    "\n",
    "asr_model = nemo_asr.models.EncDecCTCModelBPE.restore_from(\n",
    "    restore_path=\"/home/khoatlv/Conformer_ASR/models/conformer/Conformer_small_epoch=98.nemo\",\n",
    "    map_location='cuda'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2022-05-27 15:10:40 nemo_logging:349] /tmp/ipykernel_1540196/241516400.py:1: LightningDeprecationWarning: The `LightningModule.summarize` method is deprecated in v1.5 and will be removed in v1.7. Use `pytorch_lightning.utilities.model_summary.summarize` instead.\n",
      "      asr_model.summarize()\n",
      "    \n",
      "[NeMo W 2022-05-27 15:10:40 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/pytorch_lightning/utilities/model_summary.py:471: LightningDeprecationWarning: Argument `mode` in `LightningModule.summarize` is deprecated in v1.4 and will be removed in v1.6. Use `max_depth=1` to replicate `mode=top` behavior.\n",
      "      rank_zero_deprecation(\n",
      "    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'sample_rate': 16000, 'log_prediction': True, 'ctc_reduction': 'mean_batch', 'num_workers': 16, 'train_ds': {'manifest_filepath': '/home/nhan/NovaIntechs/data/ASR_Data/manifests/train_manifest_processed.json', 'sample_rate': 16000, 'max_duration': 16.7, 'min_duration': 0.1, 'is_tarred': False, 'tarred_audio_filepaths': None, 'shuffle_n': 2048, 'bucketing_strategy': 'synced_randomized', 'bucketing_batch_size': None, 'shuffle': True, 'batch_size': 32, 'pin_memory': True, 'trim_silence': True, 'use_start_end_token': True, 'normalize_transcripts': False, 'num_workers': 16}, 'validation_ds': {'manifest_filepath': '/home/nhan/NovaIntechs/data/ASR_Data/manifests/test_manifest_processed.json', 'sample_rate': 16000, 'pin_memory': True, 'shuffle': False, 'batch_size': 8, 'trim_silence': True, 'use_start_end_token': True, 'normalize_transcripts': False, 'num_workers': 16}, 'test_ds': {'manifest_filepath': '/home/nhan/NovaIntechs/data/ASR_Data/manifests/test_manifest_processed.json', 'sample_rate': 16000, 'pin_memory': True, 'shuffle': False, 'batch_size': 8, 'trim_silence': True, 'use_start_end_token': True, 'normalize_transcripts': False, 'num_workers': 16}, 'tokenizer': {'dir': '/home/nhan/NovaIntechs/src/ASR-Nemo/Nova/tokenizers/tokenizers_conformer/tokenizer_spe_bpe_v256', 'type': 'bpe', 'model_path': 'nemo:e617f9a89e7f4ba084e3cffd61db7077_tokenizer.model', 'vocab_path': 'nemo:7e04139338714e09aa4ff064de87ae7b_vocab.txt', 'spe_tokenizer_vocab': 'nemo:2f4906755f1247f2b3756468793d6f88_tokenizer.vocab'}, 'preprocessor': {'_target_': 'nemo.collections.asr.modules.AudioToMelSpectrogramPreprocessor', 'sample_rate': 16000, 'normalize': 'per_feature', 'window_size': 0.025, 'window_stride': 0.01, 'window': 'hann', 'features': 80, 'n_fft': 512, 'log': True, 'frame_splicing': 1, 'dither': 1e-05, 'pad_to': 0, 'pad_value': 0.0}, 'spec_augment': {'_target_': 'nemo.collections.asr.modules.SpectrogramAugmentation', 'freq_masks': 2, 'time_masks': 2, 'freq_width': 27, 'time_width': 0.05}, 'encoder': {'_target_': 'nemo.collections.asr.modules.ConformerEncoder', 'feat_in': 80, 'feat_out': -1, 'n_layers': 16, 'd_model': 176, 'subsampling': 'striding', 'subsampling_factor': 4, 'subsampling_conv_channels': -1, 'ff_expansion_factor': 4, 'self_attention_model': 'rel_pos', 'n_heads': 4, 'att_context_size': [-1, -1], 'xscaling': True, 'untie_biases': True, 'pos_emb_max_len': 5000, 'conv_kernel_size': 31, 'conv_norm_type': 'batch_norm', 'dropout': 0.1, 'dropout_emb': 0.0, 'dropout_att': 0.1}, 'decoder': {'_target_': 'nemo.collections.asr.modules.ConvASRDecoder', 'feat_in': 176, 'num_classes': 256, 'vocabulary': ['<unk>', '▁c', '▁t', 'ng', 'nh', '▁đ', '▁l', '▁m', '▁th', '▁v', '▁ch', '▁h', '▁b', '▁n', '▁nh', '▁k', '▁là', '▁s', '▁tr', '▁kh', '▁ng', '▁g', '▁cá', 'ông', '▁r', '▁p', '▁ph', '▁d', '▁cái', '▁gi', '▁có', 'ình', 'ời', '▁và', 'ên', '▁thì', 'qu', '▁qu', '▁không', 'iệ', '▁mà', 'ột', '▁một', 'ôi', 'ới', 'ất', 'ủa', '▁x', '▁của', '▁như', '▁nó', 'ười', '▁a', '▁đư', 'em', 'iế', '▁nà', '▁người', '▁mình', '▁em', 'ại', 'uy', '▁cho', '▁đi', 'ợc', 'ững', '▁những', 'ay', '▁được', '▁tôi', 'ấy', '▁ngh', '▁đó', '▁co', 'ong', 'ũng', 'ải', 'ai', '▁các', '▁cũng', 'ều', '▁anh', '▁chú', '▁con', '▁với', 'ươ', 'ây', 'ân', '▁ta', 'ác', 'ồi', 'ạn', 'ần', '▁trong', '▁để', '▁này', '▁phải', 'ao', '▁cả', '▁đã', '▁làm', '▁sẽ', '▁nhi', '▁chúng', '▁rất', 'ướ', 'ến', 'ăm', '▁bạn', '▁ra', 'uố', '▁rồi', 'ương', '▁à', 'ang', '▁lại', 'ờng', '▁ở', '▁hai', '▁về', 'inh', 'âu', 'an', '▁thể', 'au', 'ết', '▁việ', '▁á', '▁đến', '▁họ', '▁ti', '▁nói', 'ước', 'ành', '▁khi', 'iết', 'ơn', '▁gì', '▁nhưng', 'ầu', '▁vậ', '▁nhiều', '▁chị', 'iện', 'ài', '▁thế', 'ch', '▁giờ', 'òn', '▁nào', 'ăn', 'ường', '▁ho', 'àn', 'ằng', 'ức', 'ữa', '▁còn', 'úc', '▁thấy', '▁chuy', '▁từ', '▁', 'n', 'h', 'c', 't', 'i', 'g', 'm', 'à', 'a', 'đ', 'u', 'l', 'o', 'á', 'ư', 'y', 'v', 'r', 'b', 'ó', 'ô', 'ì', 'k', 'p', 's', 'ờ', 'ế', 'ạ', 'ấ', 'ả', 'ộ', 'ê', 'â', 'ớ', 'ệ', 'd', 'ố', 'ề', 'ơ', 'e', 'ể', 'ú', 'ủ', 'ợ', 'ữ', 'q', 'ị', 'ậ', 'x', 'ầ', 'ồ', 'ọ', 'ă', 'í', 'ứ', 'ở', 'ắ', 'ã', 'ự', 'ũ', 'ò', 'ừ', 'ụ', 'ẽ', 'ặ', 'ù', 'ổ', 'ý', 'ỏ', 'ĩ', 'ẹ', 'ằ', 'ỉ', 'é', 'ử', 'ẫ', 'ỗ', 'ẻ', 'ễ', 'ẩ', 'è', 'ỡ', 'õ', 'ẳ', 'ỹ', 'ỳ', 'ỷ', 'ẵ', 'ỵ', 'w', 'f', 'z', 'j']}, 'optim': {'name': 'novograd', 'lr': 0.025, 'betas': [0.8, 0.25], 'weight_decay': 0.001, 'sched': {'name': 'CosineAnnealing', 'warmup_steps': None, 'warmup_ratio': 0.1, 'min_lr': 1e-09, 'last_epoch': -1}}, 'target': 'nemo.collections.asr.models.ctc_bpe_models.EncDecCTCModelBPE', 'nemo_version': '1.7.0rc0'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asr_model.summarize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# freeze_encoder = False \n",
    "# freeze_encoder = bool(freeze_encoder)\n",
    "\n",
    "# if freeze_encoder:\n",
    "#     asr_model.encoder.freeze()\n",
    "#     asr_model.encoder.apply(enable_bn_se)\n",
    "#     print(\"Model encoder has been frozen, and batch normalization has been unfrozen\")\n",
    "# else:\n",
    "#     asr_model.encoder.unfreeze()\n",
    "#     print(\"Model encoder has been un-frozen\")\n",
    "\n",
    "config = load_config(model_config)\n",
    "\n",
    "# Set tokenizer config\n",
    "asr_model.cfg.tokenizer.dir = TOKENIZER_DIR\n",
    "asr_model.cfg.tokenizer.type = TOKENIZER_TYPE_CFG\n",
    "\n",
    "asr_model.setup_training_data(config.model.train_ds)\n",
    "asr_model.setup_validation_data(config.model.validation_ds)\n",
    "asr_model.setup_multiple_test_data(config.model.test_ds)\n",
    "\n",
    "with open_dict(asr_model.cfg):\n",
    "    asr_model.cfg.optim = config.model.optim\n",
    "    asr_model.cfg.spec_augment = config.model.spec_augment    \n",
    "    \n",
    "asr_model.spec_augmentation = asr_model.from_config_dict(config.model.spec_augment)\n",
    "asr_model.setup_optimization(config.model.optim)\n",
    "\n",
    "asr_model._wer.use_cer = True\n",
    "asr_model._wer.log_prediction = True\n",
    "\n",
    "trainer = ptl.Trainer(**config.trainer)\n",
    "asr_model.set_trainer(trainer)\n",
    "asr_model.cfg = asr_model._cfg\n",
    "\n",
    "exp_config = exp_manager.ExpManagerConfig(**config.exp_manager)\n",
    "exp_config = OmegaConf.structured(exp_config)\n",
    "logdir = exp_manager.exp_manager(trainer, exp_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def analyse_ctc_failures_in_model(model):\n",
    "#     count_ctc_failures = 0\n",
    "#     am_seq_lengths = []\n",
    "#     target_seq_lengths = []\n",
    "\n",
    "#     device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "#     model = model.to(device)\n",
    "#     mode = model.training\n",
    "    \n",
    "#     train_dl = model.train_dataloader()\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#       model = model.eval()\n",
    "#       for batch in tqdm(train_dl, desc='Checking for CTC failures'):\n",
    "#           x, x_len, y, y_len = batch\n",
    "#           x, x_len = x.to(device), x_len.to(device)\n",
    "#           x_logprobs, x_len, greedy_predictions = model(input_signal=x, input_signal_length=x_len)\n",
    "\n",
    "#           # Find how many CTC loss computation failures will occur\n",
    "#           for xl, yl in zip(x_len, y_len):\n",
    "#               if xl <= yl:\n",
    "#                   count_ctc_failures += 1\n",
    "\n",
    "#           # Record acoustic model lengths=\n",
    "#           am_seq_lengths.extend(x_len.to('cpu').numpy().tolist())\n",
    "\n",
    "#           # Record target sequence lengths\n",
    "#           target_seq_lengths.extend(y_len.to('cpu').numpy().tolist())\n",
    "          \n",
    "#           del x, x_len, y, y_len, x_logprobs, greedy_predictions\n",
    "    \n",
    "#     if mode:\n",
    "#       model = model.train()\n",
    "      \n",
    "#     return count_ctc_failures, am_seq_lengths, target_seq_lengths\n",
    "\n",
    "# results = analyse_ctc_failures_in_model(asr_model)\n",
    "# num_ctc_failures, am_seq_lengths, target_seq_lengths = results\n",
    "# if num_ctc_failures > 0:\n",
    "#   print(f\"\\nCTC loss will fail for {num_ctc_failures} samples ({num_ctc_failures * 100./ float(len(am_seq_lengths))} % of samples)!\\n\"\n",
    "#                   f\"Increase the vocabulary size of the tokenizer so that this number becomes close to zero !\")\n",
    "# else:\n",
    "#   print(\"No CTC failure cases !\")\n",
    "# # Compute average ratio of T / U\n",
    "# avg_T = sum(am_seq_lengths) / float(len(am_seq_lengths))\n",
    "# avg_U = sum(target_seq_lengths) / float(len(target_seq_lengths))\n",
    "\n",
    "# avg_length_ratio = 0\n",
    "# for am_len, tgt_len in zip(am_seq_lengths, target_seq_lengths):\n",
    "#   avg_length_ratio += (am_len / float(tgt_len))\n",
    "# avg_length_ratio = avg_length_ratio / len(am_seq_lengths)\n",
    "\n",
    "# print(f\"Average Acoustic model sequence length = {avg_T}\")\n",
    "# print(f\"Average Target sequence length = {avg_U}\")\n",
    "# print()\n",
    "# print(f\"Ratio of Average AM sequence length to target sequence length = {avg_length_ratio}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Train the model\n",
    "# trainer.fit(asr_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save checkpoint to nemo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2022-05-01 11:40:46 mixins:146] Tokenizer SentencePieceTokenizer initialized with 256 tokens\n",
      "[NeMo I 2022-05-01 11:40:46 ctc_bpe_models:206] \n",
      "    Replacing placeholder number of classes (-1) with actual number of classes - 256\n",
      "[NeMo I 2022-05-01 11:40:54 collections:173] Dataset loaded with 169524 files totalling 185.88 hours\n",
      "[NeMo I 2022-05-01 11:40:54 collections:174] 1589 files were filtered totalling 10.03 hours\n",
      "[NeMo I 2022-05-01 11:40:55 collections:173] Dataset loaded with 19050 files totalling 21.30 hours\n",
      "[NeMo I 2022-05-01 11:40:55 collections:174] 0 files were filtered totalling 0.00 hours\n",
      "[NeMo I 2022-05-01 11:40:56 collections:173] Dataset loaded with 19050 files totalling 21.30 hours\n",
      "[NeMo I 2022-05-01 11:40:56 collections:174] 0 files were filtered totalling 0.00 hours\n",
      "[NeMo I 2022-05-01 11:40:56 features:255] PADDING: 0\n",
      "[NeMo I 2022-05-01 11:40:56 features:272] STFT using torch\n"
     ]
    }
   ],
   "source": [
    "# config = load_config(model_config)\n",
    "# config.model.tokenizer.dir = TOKENIZER_DIR\n",
    "# config.model.tokenizer.type = TOKENIZER_TYPE_CFG\n",
    "\n",
    "# asr_model = nemo_asr.models.EncDecCTCModelBPE(config.model)\n",
    "# checkpoint = torch.load(\n",
    "#     \"/home/khoatlv/Conformer_ASR/experiments/Conformer_small_Model_Language_vi/2022-04-08_04-39-29/checkpoints/Conformer_small_Model_Language_vi--val_wer=0.0438-epoch=47.ckpt\",\n",
    "#     map_location='cuda'\n",
    "# )\n",
    "# asr_model.load_state_dict(state_dict=checkpoint['state_dict'])\n",
    "# asr_model.save_to(\"/home/khoatlv/Conformer_ASR/models/conformer/Conformer_small_epoch=98+20+11+47=176.nemo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# asr_model = nemo_asr.models.EncDecCTCModelBPE.restore_from(\n",
    "#     \"/home/khoatlv/Conformer_ASR/models/conformer/Conformer_small_epoch=98+20+11+47+10=186.nemo\",\n",
    "#     map_location='cuda'\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# audio_name = \"FPTOpenSpeechData_Set002_V0.1_011692.wav\"\n",
    "# AUDIO_FILENAME = os.path.join(\"/home/khoatlv/data/FPT/wav\", audio_name)\n",
    "\n",
    "# text = asr_model.transcribe([AUDIO_FILENAME])\n",
    "# print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
